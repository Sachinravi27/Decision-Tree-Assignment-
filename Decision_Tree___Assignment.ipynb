{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree | Assignment**\n",
        "\n",
        "**Question 1:** What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "  - A Decision Tree is a supervised learning algorithm used for both classification and regression. In classification, it works by splitting the dataset into smaller groups based on feature values, forming a tree-like structure.\n",
        "\n",
        "  - Each internal node represents a test on a feature (for example, “age > 30?”), each branch is the outcome of the test (Yes/No), and each leaf node represents the final class label.\n",
        "\n",
        "  - The tree is built using measures like Gini impurity or Entropy, which decide the best feature to split on at each step. The process continues until the data is well separated or a stopping condition (like maximum depth) is reached.\n",
        "\n",
        "**Question 2:** Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "  -Decision Trees use impurity measures to decide which feature to split on at each step. The two common measures are Gini Impurity and Entropy.\n",
        "\n",
        "   - Gini Impurity:Measures how often a randomly chosen sample would be misclassified if it were labeled according to the class distribution in a node.\n",
        "\n",
        "  - Value = 0 → all samples belong to one class (pure node).\n",
        "\n",
        " -   Higher value → more mixed classes.\n",
        "\n",
        " - Entropy: Measures the disorder or uncertainty in a node.\n",
        "\n",
        "  - Value = 0 → node is pure.\n",
        "\n",
        "  - Higher value → node contains more mixed classes.\n",
        "\n",
        "  - Impact on splits: When building a tree, the algorithm looks for the split that reduces impurity the most.\n",
        "\n",
        "  - Using Gini or Entropy, the best split creates child nodes that are more pure than the parent node.\n",
        "\n",
        "  - This ensures the tree separates the classes effectively, improving classification accuracy.\n",
        "\n",
        "**Question 3:** What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "\n",
        "  - Pruning is a way to control the growth of a Decision Tree so it does not become too complex. There are two types:\n",
        "\n",
        "  - Pre-Pruning (Early Stopping): Here, we stop the tree from growing too deep right at the training stage. For example, we can set limits like maximum depth, minimum samples per leaf, or minimum information gain.\n",
        "\n",
        "  - Advantage: It saves time and reduces the risk of overfitting because the tree never grows unnecessarily large.\n",
        "\n",
        "  - Post-Pruning (Pruning After Training): In this method, we first grow the tree fully and then remove branches that do not add much value. The idea is to simplify the model while keeping accuracy close to the original.\n",
        "\n",
        "  - Advantage: It often gives better accuracy compared to pre-pruning because the tree had the chance to learn all possible splits before being simplified.\n",
        "\n",
        "**Question 4:** What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "  - Information Gain measures how much uncertainty or impurity is reduced after splitting a node in a Decision Tree.\n",
        "\n",
        "It is calculated as:\n",
        "\n",
        "  - **Information Gain = Impurity of parent node - (Weighted sum of impurities of child nodes)**\n",
        "\n",
        "  - A high Information Gain means the split creates purer child nodes.\n",
        "\n",
        "  - A low Information Gain means the split does not help much in separating classes.\n",
        "\n",
        "  - Why it is important: Decision Trees use Information Gain to choose the best feature and value to split on. By always selecting the split with the highest Information Gain, the tree improves its ability to classify data correctly and makes more accurate predictions.\n",
        "\n",
        "\n",
        "**Question 5:** What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "  - Real-world applications:\n",
        "\n",
        "Banking: Predicting whether a customer will default on a loan.\n",
        "Healthcare: Diagnosing diseases based on patient symptoms and test results.\n",
        "\n",
        "E-commerce: Predicting whether a customer will buy a product or respond to a marketing campaign.\n",
        "\n",
        "Finance: Fraud detection by classifying transactions as normal or suspicious.\n",
        "\n",
        "  - Advantages:\n",
        "\n",
        "Easy to understand and visualize — even non-technical people can follow the decisions.\n",
        "\n",
        "Can handle both numerical and categorical data.\n",
        "\n",
        "No need to scale or normalize features.\n",
        "\n",
        "  - Limitations:\n",
        "\n",
        "Can overfit if the tree grows too deep.\n",
        "\n",
        "Small changes in data can lead to completely different trees.\n",
        "\n",
        "Usually less accurate than ensemble methods like Random Forest or Gradient Boosting.\n",
        "\n"
      ],
      "metadata": {
        "id": "yvhzl3JGlUsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances '''\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset (built-in in sklearn)\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # features\n",
        "y = data.target  # labels\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(X.columns, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crC57BB1rmyJ",
        "outputId": "95adde4e-7974-4496-faaf-4fa7a2ca001f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree. '''\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully-grown Decision Tree (no depth limit)\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracy with max_depth=3:\", acc_limited)\n",
        "print(\"Accuracy with fully-grown tree:\", acc_full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1tHWzpZr__r",
        "outputId": "a58a5390-081b-4285-cbe6-7deffb3bbfd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with fully-grown tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Question 8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances '''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(X.columns, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_O5bmdLsVuB",
        "outputId": "ffe2b868-dc53-49b3-ef79-d65cd31a3c6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy '''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "best_params = grid.best_params_\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Accuracy with best parameters:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0fxa-V8sjCA",
        "outputId": "69298f49-ed47-41be-fa54-3133c0584db5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy with best parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting. '''\n",
        "\n",
        "'''Answer :  Handle Missing Values:\n",
        "\n",
        "First, check which columns have missing values.\n",
        "\n",
        "For numerical features, fill missing values with the mean or median.\n",
        "\n",
        "For categorical features, fill missing values with the mode (most common category) or use a special category like “Unknown”.\n",
        "\n",
        "This ensures the model can process all data without errors.\n",
        "\n",
        "Encode Categorical Features:\n",
        "\n",
        "Convert categorical features into numerical form since Decision Trees in sklearn require numeric input.\n",
        "\n",
        "For features with no order, use one-hot encoding.\n",
        "\n",
        "For features with ordinal order, use label encoding.\n",
        "\n",
        "Train a Decision Tree Model:\n",
        "\n",
        "Split the dataset into training and test sets.\n",
        "\n",
        "Train a Decision Tree Classifier, using a criterion like Gini or Entropy.\n",
        "\n",
        "Initially, you can use default parameters to see basic performance.\n",
        "\n",
        "Tune Hyperparameters:\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV to find the best parameters.\n",
        "\n",
        "Important parameters include:\n",
        "\n",
        "max_depth → limits tree depth to prevent overfitting\n",
        "\n",
        "min_samples_split → minimum samples required to split a node\n",
        "\n",
        "min_samples_leaf → minimum samples required at a leaf node\n",
        "\n",
        "max_features → number of features considered for each split\n",
        "\n",
        "This helps make the model both accurate and simple.\n",
        "\n",
        "Evaluate Performance:\n",
        "\n",
        "Use metrics like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "In healthcare, recall is especially important because missing a patient with a disease is risky.\n",
        "\n",
        "Use a confusion matrix to see false positives and false negatives.\n",
        "\n",
        "Business Value:\n",
        "\n",
        "This model can help early detection of diseases, allowing doctors to intervene sooner.\n",
        "\n",
        "It can prioritize high-risk patients for further tests.\n",
        "\n",
        "Helps the healthcare company reduce costs by avoiding unnecessary tests and focusing resources efficiently.\n",
        "\n",
        "The model can also identify important risk factors, giving insights into which features contribute most to the disease. '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "lvFm4NwXs5Rr",
        "outputId": "6852bff3-a433-4330-f3d3-cf8b25a8b73b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer Handle Missing Values:\\n\\nFirst, check which columns have missing values.\\n\\nFor numerical features, fill missing values with the mean or median.\\n\\nFor categorical features, fill missing values with the mode (most common category) or use a special category like “Unknown”.\\n\\nThis ensures the model can process all data without errors.\\n\\nEncode Categorical Features:\\n\\nConvert categorical features into numerical form since Decision Trees in sklearn require numeric input.\\n\\nFor features with no order, use one-hot encoding.\\n\\nFor features with ordinal order, use label encoding.\\n\\nTrain a Decision Tree Model:\\n\\nSplit the dataset into training and test sets.\\n\\nTrain a Decision Tree Classifier, using a criterion like Gini or Entropy.\\n\\nInitially, you can use default parameters to see basic performance.\\n\\nTune Hyperparameters:\\n\\nUse GridSearchCV or RandomizedSearchCV to find the best parameters.\\n\\nImportant parameters include:\\n\\nmax_depth → limits tree depth to prevent overfitting\\n\\nmin_samples_split → minimum samples required to split a node\\n\\nmin_samples_leaf → minimum samples required at a leaf node\\n\\nmax_features → number of features considered for each split\\n\\nThis helps make the model both accurate and simple.\\n\\nEvaluate Performance:\\n\\nUse metrics like accuracy, precision, recall, and F1-score.\\n\\nIn healthcare, recall is especially important because missing a patient with a disease is risky.\\n\\nUse a confusion matrix to see false positives and false negatives.\\n\\nBusiness Value:\\n\\nThis model can help early detection of diseases, allowing doctors to intervene sooner.\\n\\nIt can prioritize high-risk patients for further tests.\\n\\nHelps the healthcare company reduce costs by avoiding unnecessary tests and focusing resources efficiently.\\n\\nThe model can also identify important risk factors, giving insights into which features contribute most to the disease. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "13QwfBnTtHA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}